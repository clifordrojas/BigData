Scrum Notes
Date: 4/9/2020
CC:
caleb.davis@enhanceit.us, cliford.rojas@enhanceit.us, jeffery.alkire@enhanceit.us, weiyao.ma@enhanceit.us, alicja.ligas@enhanceit.us,jerry.hsieh@enhanceit.us

#################################################################################################
Summary of no/little progress:
Alicja: Pipeline progress: No changes since yesterday. Tasks: Same as yesterday.
Cliford: Pipeline progress: No changes since yesterday. Tasks: HBase: Issues "ERROR: KeeperErrorCode = ConnectionLoss for /hbase/master".
#################################################################################################  

#------------------------------------------------------------------#
Alicja

Scheduled Interview:
	2pm capital one glider
Pipeline Progress (TRAINING/CONTRACT):
	STAGE: ingestion
	CURRENT PROGRESS: automate API call after session expired
Tasks:
	CURRENT TASK: automating hadoop ecosystem, using global variable in bash (done), automate spark and Kafka, spark streaming
	NONE-COMPLETE: 31+
Studying:
	reading hadoop book
Time:
	Approx 3hr admin: emails, screenings, 
	3 hrs studying spark, interview prep
	2 hrs tasks, reviewing spark streaming and pipeline spark  
#------------------------------------------------------------------#
Caleb
Scheduled Interview:
*email broken*
4/9 Fannie Mae - TP??
4/9 Fannie Mae -IV@6pm4/9 Nokia - TP@??
4/9 Nokia - IV@11:30am4/10 RF AAA -TP@?
4/10 RF AAA - IV@2:30pm-4:30pm Pipeline Progress (TRAINING/CONTRACT):
	2nd piplineSTAGE: aws did kinesis stream (debuging credential errors)
 1st pipe: switch to kstream 

	CURRENT PROGRESS: installed databases, made kinesis stream, redoing kstream, connecting kineses to spark ec2 cluster, to stream to database  
	CURRENT TASK: Task 32 read Stream to databases  	NONE-COMPLETE: 32,33,34,35
Studying:
	dynamodbTime:
	3hr wrapping up installtions 
	3hrs figure out how wanted to impement kinesis, Dynamodb
2hr mettings + debuging 
Interview prepN/a interview
#------------------------------------------------------------------#
Cliford
Scheduled Interview:
	10:00 am - R2 - TP - Deloitte
	5:00 pm - R2 - IV - Deloitte

Pipeline Progress (TRAINING/CONTRACT):
	STAGE: Ingestion
	CURRENT PROGRESS: Working on Tasks instead of pipeline to get more understanding and experience of Hbase,Cassandra,MongoDB.
Tasks:
	CURRENT TASK: Task 33: Hbase issues.
	NONE-COMPLETE: 30,31,33,35
Studying:
	RDD,Dataset, Star Schema,Snowflake schema,Partitioning vs bucketting, spark parallelism, Create dataframes
Time:
	8-10: Scrum Notes,Schedule Check, Hbase Troubleshooting
	10-12: Q & A Prep, Tech prep-prep,Hbase fixed and running, Tech Prep (Israil)
	12-1: Lunch
	1-130: Part 1 Q & A //Vendor call in the middle of the conference.
	145:220: Part 2 Q & A
	2:20-3: Inverview Prep - RDD,DATAFRAME,DATASET, STAR vs SNOWFLAKE Schema.
	break 10min 
	3-4: Interview Prep - Hive create table, Partitioning, Bucketing,Spark partitions, spark parallelism,
			ways to create dataframes.
	4-420:Admin: Figuring out the note taker for iv. Switching back to Windows OS. Updating GitHub
	Interview
#------------------------------------------------------------------#
Jerry
Scheduled Interview:
	none
Pipeline Progress (TRAINING/CONTRACT):
	STAGE: spark read s3 file
	CURRENT PROGRESS: AWS4-HMAC-SHA256 version issue, same as move data from hdfs to s3
    Time: 2 hours on CDM
          2 hours on studyind aws
          3 hours on troubleshooting spark reading s3 file
Tasks:
	CURRENT TASK: none
	NONE-COMPLETE: Task #33 - 35
Studying:
	aws
#------------------------------------------------------------------#
Miya
Scheduled Interview: one video call at 1pm
Tasks:
	CURRENT TASK: retest SQL code on HIVE, work on code for spark Dstream, offset, window ..
	NONE-COMPLETE: T31,T33-T35
Time:
    -3hrs listen in interview and take note, listen in vendor calls and call vendors.
    -2hrs Review interview Qs and attend discussion, watched spark videos.
    -3hrs Test SQL Window functions on HIVE, wrote scala code for sparking and D-stream word count and debug.
