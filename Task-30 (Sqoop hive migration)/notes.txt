#Log in to HortonWorks
ssh maria_dev@localhost -p 2222

#Transfer Data to Maria_dev
scp your_file user@ip_address:/file/path

#Star hive 
hive
#If hive hangs use:
hive --hiveconf hive.execution.engine=mr #-f load_data.sql

#-f is run query from files: "hive -f /somepath/queries.hql"
export ACCUMULO_HOME='/usr/hdp/2.6.5.0-292/pig/doc/api/org/apache/pig/backend/hadoop/accumulo'

TODO:
•         Done- Create new Hive database movielens_db
•         Done -create 1 managed table in Hive
•         Done - create 4 tables in mySQL (and insert data )
•         Done - *move data from mySQL tables to HDFS Hive table (already created)
•         move data from mySQL to HDFS (Hive external tables 'maria_dev')
•         *create new table in mySQL
•         *move data from table back to mySQL

#-------------------------------HIVE Database------------------------------#
CREATE DATABASE movielens_db;

#Use Table
USE movielens_db;
#------------------------------Hive Tables---------------------------------#
#Create Table
CREATE TABLE hive_data
    ( user_id   INT,
      movie_id  INT,
      rating    INT,
      time      INT
    )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE u_info
    ( num_users   int,
      items  int,
      rating    int
      );

CREATE TABLE u_genre 
( genre  VARCHAR(25),
  genre_id int
);

CREATE TABLE u_occupation 
( occupation VARCHAR(50)
);
#-----------------------------Hive delete rows-------------------------------------#
insert overwrite table hive_data 
    select * from hive_data
    where user_id <> 1
;

#-----------------------------MySQL-----------------------------------------------#
#MySQL
#Create 4 Tables in MySQL
CREATE DATABASE cliffsql_db;

USE cliffsql_db;

#Table num1
CREATE TABLE hive_data
    ( user_id   int,
      movie_id  int,
      rating    int,
      time      int
    );

LOAD DATA LOCAL INFILE '/data/u.data' INTO TABLE hive_data FIELDS TERMINATED BY '\t';

#----------------------u_info table--------------------------------------------------#
CREATE TABLE u_info
    ( count   int,
      section  VARCHAR(50)
      );
# 
LOAD DATA LOCAL INFILE '/data/u.info' INTO TABLE u_info FIELDS TERMINATED BY ' ';
  
#----------------------u.genre--------------------------------------------------------#
CREATE TABLE u_genre 
( genre  VARCHAR(25),
  genre_id int
);
LOAD DATA LOCAL INFILE '/data/u.genre' INTO TABLE u_genre FIELDS TERMINATED BY '|';

#----------------------u.occupation--------------------------------------------------#
CREATE TABLE u_occupation 
( occupation VARCHAR(50)
);
LOAD DATA LOCAL INFILE '/data/u.occupation' INTO TABLE u_occupation;

#----------------------Delete hive table--------------------------#


rows---------------------------------------------------#
delete from table where column = 0;
delete from movielens_db.hive_data  where column = 0;

#Try to pass tables

#cliffsql_db

sqoop import --connect jdbc:mysql://localhost/cliffsql_db --username root -P --table hive_data --hive-import --hive-table movielens_db.hive_data -m 1

#Import to hive table 
sqoop import --connect jdbc:mysql://localhost/cliffsql_db --username maria_dev --password maria_dev --table hive_data  --hive-import  --hive-table movielens_db.hive_data -m 1

#Export to mysql
sqoop export  --connect jdbc:mysql://localhost/cliffsql_db --username maria_dev --password maria_dev --table hive_data --export-dir /user/hive/hive_data --m 1

#Run check 
hive
use movielens_db;
select * from movielens_db.hive_data limit 10;

#Remove data if failure
hdfs dfs -rm -r /user/maria_dev/hive_data

#Update hive-site.xml
<property>
   <name>hive.warehouse.subdir.inherit.perms</name>
   <value>false</value>
</property>





#-----------------------------Notes for one on one-----------------------------#

sqoop command not functional inserts null values 


